{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "skip-gram",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK85zKGI6KcX",
        "colab_type": "text"
      },
      "source": [
        "# skip-gram\n",
        "\n",
        "Word2Vec 모델 중 하나인 **skip-gram**을 파이토치로 구현합니다. 《밑바닥부터 시작하는 딥러닝 2》을 참고했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkvbmA2dUgu0",
        "colab_type": "text"
      },
      "source": [
        "## PTB 데이터 불러오기\n",
        "\n",
        "**펜 트리뱅크**(Penn Treebank, PTB) 데이터셋. word2vec 발명자인 토마스 미콜로프(Tomas Mikolov) 웹 페이지에서 받을 수 있습니다. 원래의 PTB 문장에 몇 가지 전처리가 되어있습니다. 희소한 단어는 `<unk>`로 치환되어 있다던가, 구체적인 숫자는 `N`으로 대체되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NggcSlSH6KcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "if 'ptb.train.txt' in os.listdir():\n",
        "    with open(\"ptb.train.txt\", 'r') as f:\n",
        "        text = f.read()        \n",
        "else:\n",
        "    from urllib.request import urlopen\n",
        "    url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'\n",
        "    html = urlopen(url)\n",
        "    text = html.read().decode()\n",
        "\n",
        "    with open(\"ptb.train.txt\", 'w') as f:\n",
        "        f.write(text)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyHnnAv66Kce",
        "colab_type": "text"
      },
      "source": [
        "## 훈련셋 만들기\n",
        "\n",
        "윈도우 내의 단어로 가운데 단어를 추론합니다. 여기서의 함수는 skip-gram 모델에서도 사용할 수 있습니다. skip-gram에서는 단순히 맥락과 타깃을 바꿔주기만 하면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA9Tu0GDZsS_",
        "colab_type": "text"
      },
      "source": [
        "### 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhfQWvJt6Kci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 전처리\n",
        "def preprocess(text):\n",
        "    \n",
        "    words = text.replace('\\n', '<eos>').strip().split()\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "    \n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "    \n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "# 훈련셋 만들기\n",
        "def create_contexts_target(corpus, window_size=1):\n",
        "    target = corpus[window_size:-window_size]\n",
        "    contexts = []\n",
        "    \n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size+1):\n",
        "            if t == 0:\n",
        "                continue\n",
        "            cs.append(corpus[idx+t])\n",
        "        contexts.append(cs)\n",
        "        \n",
        "    return np.array(contexts), np.array(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWCkxZNfZpW1",
        "colab_type": "text"
      },
      "source": [
        "### 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqvfkMfPZrV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 5\n",
        "\n",
        "# 훈련셋 만들기\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "contexts, target = create_contexts_target(corpus, window_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJv6I5daWEng",
        "colab_type": "text"
      },
      "source": [
        "## skip-gram, 훈련 모델 정의하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAJSHL6W6Kcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn, optim \n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "# 모델을 훈련할 때 사용할 클래스\n",
        "class Trainer():\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = model.to(self.device)\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_list = []\n",
        "        self.eval_interval = None        \n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def fit(self, x, y, max_epoch=10, batch_size=32, verbose=True):\n",
        "        x = torch.tensor(x)\n",
        "        y = torch.tensor(y)\n",
        "        dataset = TensorDataset(x, y)\n",
        "        loader = DataLoader(dataset, batch_size, shuffle=True, drop_last=True)\n",
        "        max_iters = len(loader)\n",
        "\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "        for epoch in range(max_epoch):\n",
        "            running_loss = 0\n",
        "            for iters, (xx, yy) in enumerate(loader):\n",
        "                xx = xx.to(self.device)\n",
        "                yy = yy.to(self.device)\n",
        "                loss = self.model(xx, yy, self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            self.current_epoch += 1\n",
        "\n",
        "            # 평가\n",
        "            if verbose:\n",
        "                avg_loss = total_loss / max_iters\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.3f'\n",
        "                        % (self.current_epoch, iters + 1, max_iters, elapsed_time, avg_loss))\n",
        "                self.loss_list.append(avg_loss)\n",
        "                total_loss = 0\n",
        "\n",
        "    def plot(self):\n",
        "        x = np.arange(len(self.loss_list))\n",
        "        plt.plot(x, self.loss_list, label='train')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "# 네거티브 샘플링을 위한 클래스\n",
        "class UnigramSampler():\n",
        "    def __init__(self, corpus, power=0.75, sample_size=5):\n",
        "        self.sample_size = sample_size\n",
        "        corpus = torch.tensor(corpus)\n",
        "        self.weight = torch.bincount(corpus).float() ** power\n",
        "    \n",
        "    def get_negative_sample(self, y):\n",
        "        # 부정적 예에 타깃이 포함될 수 있다.\n",
        "        # 계산량과 구현상 편의를 위해 아래와 같이 구현\n",
        "        n_samples = torch.multinomial(self.weight, y.nelement()*self.sample_size, replacement=True)\n",
        "        n_samples = n_samples.view(len(y), -1)\n",
        "        return n_samples\n",
        "\n",
        "# skip-gram 모델\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, corpus, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        V = len(np.unique(corpus))\n",
        "        H = hidden_size\n",
        "        \n",
        "        # 계층 생성\n",
        "        self.sampler = UnigramSampler(corpus)\n",
        "        self.emb1 = nn.Embedding(V, H)\n",
        "        self.emb2 = nn.Embedding(V, H)\n",
        "        nn.init.normal_(self.emb1.weight, 0, 0.01)\n",
        "        nn.init.normal_(self.emb2.weight, 0, 0.01)\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "        \n",
        "    def forward(self, x, y, device='cpu'):\n",
        "\n",
        "        h = self.emb1(x) # batch size, embedding dim\n",
        "        batch_size, hidden_dim = h.shape\n",
        "        \n",
        "        # 긍정 - CBOW와의 차이가 여기에 있다\n",
        "        target_W = self.emb2(y)                                               # batch size, target size, embedding dim\n",
        "        p_scores = torch.sum(h.view(batch_size, 1, hidden_dim) * target_W, 2) # batch size, target size\n",
        "        p_loss = self.loss_fn(p_scores, torch.ones(p_scores.shape, device=device))\n",
        "\n",
        "        # 부정\n",
        "        n_samples = self.sampler.get_negative_sample(y).to(device)            # batch size, negative sample size\n",
        "        target_W = self.emb2(n_samples)                                       # batch size, negative sample size, embedding dim\n",
        "        n_scores = torch.sum(h * target_W.transpose(0, 1), 2) # negative sample size, batch size\n",
        "        n_loss = self.loss_fn(n_scores, torch.zeros(n_scores.shape, device=device))\n",
        "        \n",
        "        # 평균 손실\n",
        "        loss = (n_loss + p_loss) / (p_scores.nelement() + n_scores.nelement())\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def word_vecs(self):\n",
        "        return self.emb1.weight.data.cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A5-73kCZhjH",
        "colab_type": "text"
      },
      "source": [
        "### 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i2A5IOI-HYN",
        "colab_type": "code",
        "outputId": "2f0dffc6-4c95-47a9-a76d-51484b0bdda1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "hidden_size = 100\n",
        "batch_size = 128\n",
        "max_epoch = 15\n",
        "verbose = True\n",
        "\n",
        "# 모델 생성\n",
        "model = SkipGram(corpus, hidden_size)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "# target과 contexts를 CBOW와는 반대로 넣어준다.\n",
        "target, contexts = contexts, target\n",
        "trainer.fit(contexts, target, max_epoch, batch_size, verbose)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 7262 / 7262 | 시간 45[s] | 손실 0.425\n",
            "| 에폭 2 |  반복 7262 / 7262 | 시간 91[s] | 손실 0.400\n",
            "| 에폭 3 |  반복 7262 / 7262 | 시간 137[s] | 손실 0.394\n",
            "| 에폭 4 |  반복 7262 / 7262 | 시간 182[s] | 손실 0.390\n",
            "| 에폭 5 |  반복 7262 / 7262 | 시간 228[s] | 손실 0.387\n",
            "| 에폭 6 |  반복 7262 / 7262 | 시간 273[s] | 손실 0.384\n",
            "| 에폭 7 |  반복 7262 / 7262 | 시간 319[s] | 손실 0.382\n",
            "| 에폭 8 |  반복 7262 / 7262 | 시간 365[s] | 손실 0.381\n",
            "| 에폭 9 |  반복 7262 / 7262 | 시간 410[s] | 손실 0.380\n",
            "| 에폭 10 |  반복 7262 / 7262 | 시간 456[s] | 손실 0.378\n",
            "| 에폭 11 |  반복 7262 / 7262 | 시간 502[s] | 손실 0.378\n",
            "| 에폭 12 |  반복 7262 / 7262 | 시간 547[s] | 손실 0.377\n",
            "| 에폭 13 |  반복 7262 / 7262 | 시간 593[s] | 손실 0.376\n",
            "| 에폭 14 |  반복 7262 / 7262 | 시간 638[s] | 손실 0.376\n",
            "| 에폭 15 |  반복 7262 / 7262 | 시간 684[s] | 손실 0.375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHL4vfsgSn-4",
        "colab_type": "code",
        "outputId": "bc6e9ca7-9e3b-475d-957b-efff038d595e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "trainer.plot()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORyvD0kkZLVV",
        "colab_type": "text"
      },
      "source": [
        "## 평가\n",
        "\n",
        "특정 단어를 넣었을 때 유사한 단어를 가려낼 수 있는지, 그리고 단어 간 비유적 관계를 찾아낼 수 있는지를 볼 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXGnuqwoZbRt",
        "colab_type": "text"
      },
      "source": [
        "### 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7loUsSyZPIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def most_similar(query, word_to_id, id_to_word, similarity_matirx, top=5):\n",
        "    if query not in word_to_id:\n",
        "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
        "        return\n",
        "    \n",
        "    print('\\n[query] ' + query)\n",
        "    query_id = word_to_id[query]\n",
        "    similarity = similarity_matirx[query_id]\n",
        "    \n",
        "    # 코사인 유사도 기준으로 내림차순으로 출력\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "        \n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "def analogy(a, b, c, word_to_id, id_to_word, word_vecs, top=5):\n",
        "    for word in (a, b, c):\n",
        "        if word not in word_to_id:\n",
        "            print('%s(을)를 찾을 수 없습니다.' % word)\n",
        "            return\n",
        "\n",
        "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
        "    a_vec, b_vec, c_vec = word_vecs[word_to_id[a]], word_vecs[word_to_id[b]], word_vecs[word_to_id[c]]\n",
        "    query_vec = b_vec - a_vec + c_vec\n",
        "    word_vecs_norm = word_vecs / np.linalg.norm(word_vecs, axis=1, keepdims=True)\n",
        "    similarity = np.dot(word_vecs_norm, query_vec)\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] in (a, b, c):\n",
        "            continue\n",
        "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGN5fA9XZePB",
        "colab_type": "text"
      },
      "source": [
        "### 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YrDoSagVugY",
        "colab_type": "code",
        "outputId": "2e9c462c-814c-4c91-d847-cf7c6f7d739b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "word_vecs = model.word_vecs()\n",
        "similarity_matirx = cosine_similarity(word_vecs)\n",
        "\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, similarity_matirx)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[query] you\n",
            " i: 0.69232154\n",
            " your: 0.65297306\n",
            " yourself: 0.6527038\n",
            " anybody: 0.62820214\n",
            " do: 0.61197925\n",
            "\n",
            "[query] year\n",
            " month: 0.6314531\n",
            " earlier: 0.6273992\n",
            " week: 0.5166726\n",
            " decade: 0.4956562\n",
            " fiscal: 0.49133912\n",
            "\n",
            "[query] car\n",
            " cars: 0.6072693\n",
            " luxury: 0.5414275\n",
            " chevrolet: 0.5100602\n",
            " auto: 0.5083307\n",
            " corsica: 0.501513\n",
            "\n",
            "[query] toyota\n",
            " motor: 0.6842423\n",
            " lexus: 0.6364988\n",
            " honda: 0.61917984\n",
            " infiniti: 0.6056482\n",
            " nissan: 0.58814454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LFD5yBO1btq",
        "colab_type": "code",
        "outputId": "b973fdac-c245-431c-ade2-17e349b0a2c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[analogy] king:man = queen:?\n",
            " road: 2.6660079956054688\n",
            " rage: 2.601775646209717\n",
            " bikers: 2.5797839164733887\n",
            " hat: 2.520460605621338\n",
            " weather: 2.372321605682373\n",
            "\n",
            "[analogy] take:took = go:?\n",
            " went: 2.072838306427002\n",
            " stands: 1.9286367893218994\n",
            " moved: 1.8612631559371948\n",
            " trap: 1.8375418186187744\n",
            " got: 1.7516160011291504\n",
            "\n",
            "[analogy] car:cars = child:?\n",
            " children: 2.699692487716675\n",
            " audiences: 2.359595775604248\n",
            " conception: 2.312422752380371\n",
            " sleep: 2.2767601013183594\n",
            " psychiatric: 2.2132794857025146\n",
            "\n",
            "[analogy] good:better = bad:?\n",
            " those: 1.7770507335662842\n",
            " worse: 1.6371487379074097\n",
            " left: 1.5435960292816162\n",
            " involved: 1.5154612064361572\n",
            " done: 1.4858317375183105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-zuNW1LBpGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}